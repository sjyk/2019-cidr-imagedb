\section{Background and API}
As a direct consequence of the rapid progress in computer vision, visual analytics workloads are becoming increasingly complex with requirements. 
For example, it is not uncommon for visual perception pipelines in robotics to integrate predictions from multiple neural networks~\cite{hodson2018robots}.
We find that a main pain point is joining information from two or more visual data streams.
Without a unified data and query model, implementing ones own join strategies is a recipe for disaster.
One can easily make brittle assumptions about ordering of images or the structure of any associated metadata.
Or, one might use ad hoc data structures that will not scale past memory limits.
This echoes the challenges of the pre-relational days of database research before the idea of data independence where the query processing, the data, and the storage format are all intertwined.
We want a unified model that substantially covers much of visual analytics that can allow us to leverage logical-physical separation principles that the database community has pioneered.

\subsection{Patch Query Model}
After surveying a number of use cases from robotics to traffic camera analysis, we arrived at a simple query model, where visual analytics queries are relational queries over collections of subimages (called patches) and associated metadata about how those subimages were generated.
The query processing engine is agnostic to how those patches are generated.
They can be whole images, smaller tiled subimages, or even subimages extracted by an object detection neural networks.
In pseudocode, a \texttt{Patch} object contains a pointer to the image that generated it, the data contained in the patch, and a key-value metadata dictionary:
\begin{lstlisting}
Patch(ImgRef, Data, MetaData)
\end{lstlisting}
We are purposefully vague about the structure of \texttt{Data} and \texttt{MetaData}. We only assume that \texttt{Data} is an n-dimensional dense vector (can represent pixel content or image features) and \texttt{MetaData} has a key-value dictionary of attributes of this data.
Inspired by dataflow systems, operators in the system implement iterators over tuples of \texttt{Patch} objects:
\begin{lstlisting}
Operator(Iterator<Tuple<Patch>> in, 
         Iterator<Tuple<Patch>> out)
\end{lstlisting}
Lineage is maintained as every operator is required to update the \texttt{ImgRef} attribute to retain a lineage chain back to the original image.
To understand why we care so much about join operations, let us contrast two examples:

\subsubsection{Example 1.}
Consider a CCTV feed of a parking lot that collects and stores video.
We want to evaluate the parking lot's utilization so we want to count the number of cars in each frame of the video.
We first run all of the frames through the SSD object detection network~\cite{liu2016ssd}, which returns bounding boxes and labels for common objects detected in the image.
Each of these bounding boxes defines a patch:
\begin{lstlisting}
SSDPatch(Frame, Bbox, 
         {'label': L, 'frameno': F })
\end{lstlisting}
Over these SSD patch objects, the query of interest is well expressed in relational algebra over the metadata dictionary (a filter over labels and an aggregation over frame numbers).

\subsubsection{Example 2}
Now, suppose we are given two videos from two different cameras, we want to find all cars that appear in both videos.
The first step is exactly the same as the previous example, we break the frames up into \texttt{SSDPatch} objects.
Over the two sets of \texttt{SSDPatch}, we need to compare all of the bounding boxes and return those that are sufficiently similar in terms of image content.
What is different about this query is that it leverages both the metadata and the pixel data in the bounding boxes.

What struck us as interesting about this query is that there are a number of unresolved questions about how to actually process it efficiently.
Naively, one could compare all pairs of bounding boxes and then return those of sufficient image similarity.
Most image matching algorithms use lower dimensional features to match, so another option is to pre-compute the relevant features and build a multidimensional index over one of the sets of \texttt{SSDPatch} objects, e.g., a KD-Tree over a set of color histograms.

Additionally, what if we knew that the target vehicle was red.
It is not clear whether we would want to eagerly apply that predicate. 
The predicate is likely CPU intensive and if the matching step has a low selectivity, it might be beneficial to apply the predicate after matching.
These questions motivated us to design a DBMS-like framework to answer these sorts of queries.

\subsection{Related Work \\ and Potential Architectures}
Our work is inspired by prior work on multimedia databases, which have long acknowledged the importance of indexing strategies~\cite{yoshitaka1999survey, faloutsos2012searching}.
\textsf{DeepLens} revisits the idea of a multimedia database in the era of deep learning, where the content--both structured and unstructured--is populated by the outputs of a neural network inference pipeline.
Recent work can be best summarized as filter optimization~\cite{kang2017noscope, zhang2018ffs, anderson2018physical, jiang2018chameleon}; how to evaluate a neural network predicate as quickly as possible while satisfying an accuracy constraint.
We project into the future that the community will soon move past filters and also consider joins and more complex query operators.
Addressing the more complex visual analytics queries will require leveraging ideas from classical work on indexing strategies~\cite{faloutsos2012searching}.
However, problems in this new setting are higher dimensional, have to manage uncertainty, and have to manage the provenace of downstream data. 
This paper evaluates these tradeoffs and proposes a initial architecture for a modern VDMS.
A recent paper that we would like to highlight is a data management system for Augmented Reality~\cite{haynes2018lightdb}, and many of the ideas relating to multidimensional indexing will be very relevant.
Our main conclusion from this paper is that any succesfull VDMS system will have to have a sophisticated and robust query optimizer and some level of automated physical design. 
We envision that new results in learning-based query optimization~\cite{kaftan2018cuttlefish,krishnan2018deeprljoins} and ideas inspired by the new automated tools for physical design such as~\cite{sharma2018case,pavlo2017self} will be crucial.











